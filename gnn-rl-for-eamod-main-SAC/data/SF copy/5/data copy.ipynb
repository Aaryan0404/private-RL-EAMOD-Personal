{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 180, 12)\n",
      "(180, 180, 12)\n",
      "(180, 180, 12)\n"
     ]
    }
   ],
   "source": [
    "od_data = np.load('od_matrix.npy')\n",
    "duration_data = np.load('duration_matrix.npy')\n",
    "distance_data = np.load('distance_matrix.npy')\n",
    "\n",
    "# remove transit zones\n",
    "od_data = od_data[:-13, :-13, 8:-4]\n",
    "duration_data = duration_data[:-13, :-13, 8:-4]\n",
    "distance_data = distance_data[:-13, :-13, 8:-4]\n",
    "\n",
    "print(od_data.shape)\n",
    "print(duration_data.shape)\n",
    "print(distance_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(data, num_sets, nodes_per_set, aggregation='sum'):\n",
    "    \"\"\"\n",
    "    Aggregate data by summing or averaging over groups of nodes.\n",
    "    \n",
    "    Args:\n",
    "    - data (np.array): The input data of shape (nodes, nodes, time).\n",
    "    - num_sets (int): The number of sets (or clusters) of nodes.\n",
    "    - nodes_per_set (int): The number of nodes in each set.\n",
    "    - aggregation (str): The aggregation method ('sum' or 'average').\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: The aggregated data of shape (num_sets, num_sets, time).\n",
    "    \"\"\"\n",
    "    aggregated_data = np.zeros((num_sets, num_sets, data.shape[2]))\n",
    "    \n",
    "    for i in range(num_sets):\n",
    "        for j in range(num_sets):\n",
    "            subset = data[i*nodes_per_set:(i+1)*nodes_per_set, j*nodes_per_set:(j+1)*nodes_per_set]\n",
    "            \n",
    "            if aggregation == 'sum':\n",
    "                aggregated_data[i, j] = subset.sum(axis=(0, 1))\n",
    "            elif aggregation == 'average':\n",
    "                aggregated_data[i, j] = subset.mean(axis=(0, 1))\n",
    "    \n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 45, 12)\n",
      "(45, 45, 12)\n",
      "(45, 45, 12)\n",
      "peak demand: 11921.0\n",
      "20 of peak demand 2384.2000000000003\n",
      "cars per station 95.36800000000002\n"
     ]
    }
   ],
   "source": [
    "# Ignore the first 10 origin and destination nodes\n",
    "# Look at only the next 10 origin and destination nodes\n",
    "\n",
    "od_data = aggregate_data(od_data, 45, 4, aggregation='sum')\n",
    "duration_data = aggregate_data(duration_data, 45, 4, aggregation='average')\n",
    "distance_data = aggregate_data(distance_data, 45, 4, aggregation='average')\n",
    "\n",
    "print(od_data.shape)\n",
    "print(duration_data.shape)\n",
    "print(distance_data.shape)\n",
    "\n",
    "od_data = od_data[0:5, 5:10, :]\n",
    "duration_data = duration_data[0:5, 5:10, :]\n",
    "distance_data = distance_data[0:5, 5:10, :]\n",
    "\n",
    "sum_across_od = od_data.sum(axis=(0, 1))\n",
    "print(\"peak demand:\", max(sum_across_od.flatten()))\n",
    "\n",
    "print(\"20 of peak demand\", 0.2*max(sum_across_od.flatten()))\n",
    "print(\"cars per station\", 0.2*0.2*max(sum_across_od.flatten())/(od_data.shape[0]))\n",
    "\n",
    "# od_data = od_data[10:15, 15:20, :]\n",
    "# duration_data = duration_data[10:15, 15:20, :]\n",
    "# distance_data = distance_data[10:15, 15:20, :]\n",
    "\n",
    "# od_data = od_data[10:, 10:, :]\n",
    "# duration_data = duration_data[10:, 10:, :]\n",
    "# distance_data = distance_data[10:, 10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33744 0.33744 0.33744 0.33744 0.2909  0.2909  0.2909  0.2909  0.2909\n",
      " 0.2909  0.2909  0.2909  0.2909  0.2909  0.2909  0.2909  0.2909  0.2909\n",
      " 0.2909  0.2909  0.2909  0.2909  0.2909  0.2909  0.33744 0.33744 0.33744\n",
      " 0.33744 0.33744 0.33744 0.33744 0.33744 0.7639  0.7639  0.7639  0.7639\n",
      " 0.7639  0.7639  0.7639  0.7639  0.7639  0.7639  0.7639  0.7639  0.7639\n",
      " 0.7639  0.7639  0.7639 ]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# set remaing nans to avg\n",
    "avg_duration = int(np.nanmean(duration_data))\n",
    "duration_data[np.isnan(duration_data)] = avg_duration\n",
    "avg_distance = int(np.nanmean(distance_data))\n",
    "distance_data[np.isnan(distance_data)] = avg_distance\n",
    "\n",
    "delta_c = 2 # energy step [kWh] 0.75\n",
    "time_granularity = 0.25 # in h\n",
    "\n",
    "# cost_per_mile = 2.75 # in $\n",
    "# cost_per_hour = 33 # in $ as per https://www.taxi-calculator.com/taxi-rate-san-francisco/271\n",
    "# cost_per_timestep = cost_per_hour * time_granularity\n",
    "\n",
    "p_travel = 0.0770 # [$ / mi] https://newsroom.aaa.com/wp-content/uploads/2021/08/2021-YDC-Brochure-Live.pdf \n",
    "avg_miles_per_h_driving = 9.6 # in m/h from 190x190x24\n",
    "operational_cost_per_timestep = avg_miles_per_h_driving * time_granularity * p_travel\n",
    "\n",
    "beta = 0.6 # in $ according to https://www.bls.gov/regions/west/news-release/averageenergyprices_sanfrancisco.htm#:~:text=San%20Francisco%20area%20households%20paid,per%20therm%20spent%20last%20year -> gives cost for kwh (0.30$/kwh)) + 0.30$ for maintenance \n",
    "\n",
    "chevy_bolt_capacity = 65 # in kWh\n",
    "chevy_bolt_usable_capacity = chevy_bolt_capacity * 0.6 # never go below 20% or above 80% of charge\n",
    "charger_capacity = 50 # assuming 22.5 kW Chargers\n",
    "charge_time_per_delta_c = math.ceil((delta_c/charger_capacity)/time_granularity)\n",
    "charge_levels_per_charge_step = math.floor((charger_capacity*time_granularity)/delta_c)\n",
    "\n",
    "chevy_bolt_range = 230 # range in mi for mild city trips according to https://media.chevrolet.com/media/us/en/chevrolet/2022-bolt-euv-bolt-ev.detail.html/content/Pages/news/us/en/2021/feb/0214-boltev-bolteuv-specifications.html\n",
    "chevy_bolt_usable_range = chevy_bolt_range*0.6 # never go below 20% or above 80% of charge and assume 10% less efficient because of range https://cleantechnica.com/2017/10/13/autonomous-cars-shorter-range-due-high-power-consumption-computers/\n",
    "chevy_bolt_kwh_per_mi = (chevy_bolt_usable_capacity/chevy_bolt_usable_range)/0.7\n",
    "# print(chevy_bolt_kwh_per_mi)\n",
    "energy_distance = np.ceil(((distance_data * chevy_bolt_kwh_per_mi)/delta_c).max(axis=2))\n",
    "energy_distance[energy_distance==0] = 1 # we should always use energy to satisfy a trip\n",
    "np.save('energy_distance.npy', energy_distance)\n",
    "# print(np.sum(energy_distance==1.))\n",
    "# print(energy_distance.max())\n",
    "duration_data = np.round(duration_data/(3600*time_granularity)) # convert travel time from sec to h\n",
    "duration_data[duration_data==0] = 1. # it should always take time to satisfy a trip\n",
    "data_timespan = od_data.shape[2]\n",
    "episode_length = int(data_timespan/time_granularity)\n",
    "\n",
    "p_energy = np.ones(int(24/time_granularity))*0.16872 # in $/kWh\n",
    "p_energy[int(16/time_granularity):int(21/time_granularity)] = 0.38195 # peak prices\n",
    "p_energy[int(9/time_granularity):int(14/time_granularity)] = 0.14545 # super off peak prices\n",
    "p_energy *= delta_c # in $/ charge level\n",
    "\n",
    "p_energy = p_energy[int(8 * (1/time_granularity)):int(20 * (1/time_granularity))]\n",
    "\n",
    "print(p_energy)\n",
    "\n",
    "# fleet_size = 116616*0.088*2.5 # got number from Justin Lukes optimization with boundary:283905, without boundary:116616\n",
    "peak_demand = max(sum_across_od.flatten())\n",
    "fleet_size = peak_demand * 0.2\n",
    "number_chargelevels = int(chevy_bolt_usable_capacity/delta_c)\n",
    "number_spatial_nodes = od_data.shape[0]\n",
    "charge_locations = np.ones(number_spatial_nodes,dtype=bool).tolist()\n",
    "cars_per_station_capacity = (np.ceil(np.ones(number_spatial_nodes)*fleet_size*0.2/number_spatial_nodes)).tolist()\n",
    "print(number_chargelevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95756\n",
      "48\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "cost_per_mile = 0.91\n",
    "cost_per_timestep = time_granularity * 0.39\n",
    "\n",
    "new_tripAttr = []\n",
    "new_reb_time = []\n",
    "new_total_acc = []\n",
    "temp_demand = 0\n",
    "for origin in range(duration_data.shape[0]):\n",
    "    for destination in range(duration_data.shape[1]):\n",
    "        for ts in range(episode_length):\n",
    "            attr = defaultdict()\n",
    "            attr['time_stamp'] = ts\n",
    "            attr['origin'] = origin\n",
    "            attr['destination'] = destination\n",
    "            attr['demand'] = round(od_data[origin,destination,int(ts*time_granularity)]*time_granularity) # create equal distributed demand over granular time\n",
    "            temp_demand += attr['demand']\n",
    "            attr['price'] = cost_per_mile * distance_data[origin,destination,int(ts*time_granularity)] + cost_per_timestep * duration_data[origin,destination,int(ts*time_granularity)] + 2.20 + 2.70 # in $\n",
    "            new_tripAttr.append(attr)\n",
    "\n",
    "            reb = defaultdict()\n",
    "            reb['time_stamp'] = ts\n",
    "            reb['origin'] = origin\n",
    "            reb['destination'] = destination\n",
    "            reb['reb_time'] = int(duration_data[origin,destination,int(ts*time_granularity)])\n",
    "            new_reb_time.append(reb)\n",
    "print(temp_demand)\n",
    "for hour in range(24):\n",
    "    acc = defaultdict()\n",
    "    acc['hour'] = hour\n",
    "    acc['acc'] =  math.ceil(fleet_size)\n",
    "    new_total_acc.append(acc)\n",
    "new_data = defaultdict()\n",
    "new_data['demand'] = new_tripAttr\n",
    "new_data['rebTime'] = new_reb_time\n",
    "new_data['totalAcc'] = new_total_acc\n",
    "new_data['chargelevels'] = number_chargelevels\n",
    "new_data['spatialNodes'] = number_spatial_nodes\n",
    "new_data['chargeLevelsPerChargeStep'] = charge_levels_per_charge_step\n",
    "new_data['episodeLength'] = episode_length\n",
    "new_data['energy_prices'] = p_energy.tolist()\n",
    "new_data['chargeLocations'] = charge_locations\n",
    "new_data['carsPerStationCapacity'] = cars_per_station_capacity\n",
    "new_data['timeGranularity'] = time_granularity\n",
    "new_data['operationalCostPerTimestep'] = operational_cost_per_timestep\n",
    "new_data['peakHours'] = [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
    "\n",
    "print(episode_length)\n",
    "print(number_chargelevels)\n",
    "\n",
    "with open(f'SF_{number_spatial_nodes}.json', 'w') as f:\n",
    "    json.dump(new_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pamod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f8edb555df32b3dff9504d8c4d29eadaf231ea869cc9ae1afb4d8556d0e264c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
